## 股吧爬虫研究

### 一、请求架构与速度提升

采用Scrapy框架组织整个爬虫，预计总流量应当在1亿左右，因此日均流量要在1000万水平，每小时流量为在50万以上，为此要采取一系列措施提高爬虫速度

- 增大并发，降低延迟
- 使用高质量代理
- 提升硬件带宽
- 读取JSON接口代替解析页面
- 正则表达式代替XPATH解析页面
- 批量写入数据库

### 二、存储结构

一般来说存在以下几种内容页面

- 发帖人为普通用户， 包含题目 `<div id='zwconttbt'>` 和内容`<div id='zwconbody'><div class='stokecodec .xeditor'>` 以及图片`...<a><img>`
- 发帖人为普通用户，但包含转发内容`<div id='zwzfc'>`，转发内容一般过长，不适合存储
- 发帖人为资讯类，内容一般较长，而且`<div class='stokecodec .xeditor'>`下页面结构与普通用户不同
- 

### 三、反爬规避

#### 1. 封禁IP地址

使用代理池，详见第四节

#### 2. 返回垃圾页面

东方财富网为了提高爬取难度，使用已经被其封禁的IP地址进行访问时，并不返回404页面，而是在原请求链接上返回垃圾页面，因此必须对垃圾页面进行识别

从页面结构上来看，正常页面会显示其股票代码，垃圾页面（股市实战吧、股民学校吧）则无股票代码，一旦发现跳转到垃圾页面，则删除目前正在使用的代理，将请求URL重新放回到调度器中

#### 3. 封禁User-Agent

建立UA池（50个以上），随机选择UA作为请求标头

### 四、代理池功能

#### 1. 获取代理

每秒钟获取5个代理，设置5 * 60 * 15 = 4500作为代理池容量上限

#### 2. 检测代理

一般来说应设置检测模块以循环测试代理可用性，本代理池应当从以下三个角度考虑检测：

1. 循环访问百度，一旦发生错误，说明此代理已经失效，剔除
2. Scrapy返回垃圾页面，说明此代理已经被封禁，剔除
3. 存放在代理池中超过十五分钟的代理，剔除

#### 3. 调度模块

有些材料认为，应当按顺序循环抽取代理，以此均匀使用代理

但是，随机抽取使用代理，从概率分布上来看，也是均匀使用的

代理池本质是一个先进先出队列，使用代理的速度显然远大于获取代理的速度，估计一分钟内一个IP请求100次就会被封禁，因此需要配合随机UA一起使用